# Task 2.13 Prompt — Execute Round 2 and Generate Reports

## Persona
Senior QA engineer executing a full end-to-end evaluation run of a live RAG system, monitoring for issues, and documenting results.

## Context
- **Initiative**: enhancement--poc-evaluation
- **Phase**: Phase 2 — Systematic Testing (Layer 5: End-to-End Evaluation)
- **Dependencies**: T2.12 (evaluation harness — complete)
- **Blocks**: T3.1 (failure analysis)
- **Current state**: `scripts/evaluation_harness.py` exists and has been validated with dry-run + 2 live queries (Q-01, Q-41). Backend runs on `http://localhost:3000`. No full evaluation run has been performed yet.

### What Already Exists
- `scripts/evaluation_harness.py` — 450+ line evaluation script (Task 2.12)
- `data/evaluation_baselines.json` — 50 query baselines (Task 2.11)
- Backend: Express + ChromaDB + Groq API (Llama 3.1 8B)

### Evaluation Harness Commands
```bash
cd pilot_phase1_poc/05_evaluation

# Start backend (required - run in background)
npm start

# Full evaluation run (default 30s delay, ~25 min)
venv/Scripts/python scripts/evaluation_harness.py

# Faster run (10s delay, ~8 min) — use if Groq rate limits permit
venv/Scripts/python scripts/evaluation_harness.py --delay 10

# Resume from specific query if interrupted
venv/Scripts/python scripts/evaluation_harness.py --start-from Q-15
```

### Output Files (auto-generated by harness)
1. `data/evaluation_results.json` — full raw results with all checks
2. `data/evaluation_results.csv` — one row per query, spreadsheet-friendly
3. `reports/evaluation_report.md` — human-readable with metrics + failure breakdown

### Week 4 Targets
| Metric | Target | How Measured |
|--------|--------|-------------|
| Deflection Rate | >= 40% | % of in-scope queries where ALL must_contain keywords found |
| Citation Accuracy | >= 80% | % of in-scope queries with >= 1 source or citation |
| Hallucination Rate | < 15% | % of ALL queries with must_not_contain signal found |
| OOS Handling | >= 90% | % of OOS queries with >= 1 decline signal |
| Avg Latency | < 5s | Mean server-reported latency across all queries |

### Known Considerations from Task 2.12 Testing
1. **Cold start latency**: First query may take ~20s (ChromaDB subprocess + Groq API warm-up). This is expected — subsequent queries are faster.
2. **Citation on OOS**: OOS queries correctly have no citations, so `citation_present` marks them as FAIL. The aggregate metric only counts in-scope queries, so this is handled correctly.
3. **Q-01 citation failure**: During testing, Q-01 returned Low confidence with 0 sources. This may indicate a citation issue to investigate in Phase 3.
4. **Rate limiting**: Groq free tier allows 30 requests/minute. The default 30s delay is conservative. Try `--delay 10` first — if no 429 errors, this saves ~17 minutes.

## Task

Execute the full 50-query evaluation against the live system. This is **Round 2** — the first full-pipeline evaluation testing retrieval + generation + citation + formatting together.

### Step-by-Step Execution

#### Step 1: Ensure clean state
```bash
cd pilot_phase1_poc/05_evaluation

# Kill any stale processes
netstat -ano | findstr :3000
# If found: cmd //c "taskkill /PID <pid> /F"
```

#### Step 2: Start backend
```bash
npm start
# Verify: should see "Server started" on port 3000
```

#### Step 3: Run evaluation harness
```bash
# Activate venv
venv/Scripts/python scripts/evaluation_harness.py --delay 10
```

**Monitor for:**
- Connection refused (backend not running)
- HTTP 429 errors (increase delay if frequent)
- Query timeouts (30s timeout per query)
- Ctrl+C will save partial results

**Expected output:**
```
Loaded 50 baselines (version: 1.0)
  41 in-scope, 9 out-of-scope
Sending 50 queries to http://localhost:3000 (delay: 10s)

  [01/50] Q-01: "What documents are needed for sea freight Sin..." (1500ms) [Medium] PASS
  [02/50] Q-02: "How do I book an FCL shipment to Malaysia?" (1200ms) [High] PASS
  ...
  [50/50] Q-50: "..." (...ms) [...] PASS

Writing results...
  JSON: ...data/evaluation_results.json
  CSV:  ...data/evaluation_results.csv
  Report: ...reports/evaluation_report.md

=== EVALUATION COMPLETE ===
...metrics summary...
```

#### Step 4: Verify output files
After completion, verify all 3 output files were generated:
- `data/evaluation_results.json` — should have `run_id`, `metrics`, and 50 `results` entries
- `data/evaluation_results.csv` — should have 50 data rows + header
- `reports/evaluation_report.md` — should have aggregate metrics table, per-category breakdown, and failure details

#### Step 5: Assess results against targets
Read `reports/evaluation_report.md` and assess each metric:

| Metric | Target | Result | PASS/FAIL |
|--------|--------|--------|-----------|
| Deflection Rate | >= 40% | ?% | ? |
| Citation Accuracy | >= 80% | ?% | ? |
| Hallucination Rate | < 15% | ?% | ? |
| OOS Handling | >= 90% | ?% | ? |
| Avg Latency | < 5s | ?s | ? |

#### Step 6: Document initial observations
In the output report, capture:
1. Aggregate metrics vs targets (pass/fail per metric)
2. Per-category breakdown highlights (which categories performed best/worst)
3. Notable failures (queries that failed unexpectedly)
4. Common failure patterns (e.g., citation missing on Medium/Low confidence)
5. Recommendations for Phase 3 fix priorities

### Error Recovery

- **If harness crashes mid-run**: Use `--start-from Q-XX` to resume from the last completed query. The harness will generate reports from whatever queries completed.
- **If Groq rate-limits frequently (429)**: Increase delay to 30s: `--delay 30`
- **If backend crashes**: Restart with `npm start`, then resume harness with `--start-from`
- **If query times out (30s)**: It's logged as failed and the harness continues. Check if backend is overloaded.

## Format

- **No files to create manually** — the harness generates all 3 output files automatically
- **Output report**: `TASK_2.13_OUTPUT.md` with:
  - Run summary (timestamp, config, duration)
  - Aggregate metrics table (result vs target, pass/fail)
  - Per-category breakdown
  - Top failures and patterns
  - Phase 3 recommendations
  - Copy of the metrics from `reports/evaluation_report.md`

## Validation
- [ ] Backend started successfully on port 3000
- [ ] All 50 queries executed (no partial run)
- [ ] `data/evaluation_results.json` generated with 50 results
- [ ] `reports/evaluation_report.md` generated with metrics
- [ ] `data/evaluation_results.csv` generated with 50 rows
- [ ] Aggregate metrics calculated and documented
- [ ] No crashes during the run

## Update on Completion

**MANDATORY — Update ALL tracking locations:**
- **Checklist**: `03-checklist/IMPLEMENTATION_CHECKLIST.md` — mark Task 2.13 `[x]` AND update Phase 2 (13/13, 100%) + Total progress counts (23/43, 53%)
- **Roadmap**: `02-roadmap/IMPLEMENTATION_ROADMAP.md` — update ALL FOUR locations:
  1. **Progress Tracker** table (top) — Phase 2: 13 | 13 | Complete, Total: 43 | 23 | 53%
  2. **Quick Reference** table — change Task 2.13 status `Pending` -> `Complete`
  3. **Detailed task entry** — change `Status: Pending` -> `Status: Complete` AND check validation boxes `[x]`
  4. Phase 2 status: `In Progress` -> `Complete`
- **Bootstrap file**: `ai-workflow-bootstrap-prompt-v3.md` — update Active Initiatives progress count (23/43 -- 53%)
- **CLAUDE.md** (root) — update Active Initiatives progress count (23/43 -- 53%)
- **AGENTS.md** (root) — update Active Initiatives progress count (23/43 -- 53%)
- **Verify**: Re-read all updated files to confirm consistency
